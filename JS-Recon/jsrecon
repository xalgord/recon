#!/bin/bash

# Comprehensive JavaScript Reconnaissance Script
# Author: Security Researcher
# Description: Comprehensive JS file discovery using multiple tools

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Set timeout to 7200 seconds (2 hours)
TIMEOUT=7200

# Banner
echo -e "${CYAN}"
echo "╔══════════════════════════════════════════════════════════════════════════════╗"
echo "║                       JavaScript Reconnaissance Script                       ║"
echo "║                            Comprehensive JS Discovery                        ║"
echo "╚══════════════════════════════════════════════════════════════════════════════╝"
echo -e "${NC}"

# Function to print colored output
print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_tool() {
    echo -e "${PURPLE}[TOOL]${NC} Running $1..."
}

# Function to check if a tool is installed
check_tool() {
    if ! command -v $1 &> /dev/null; then
        print_error "$1 is not installed or not in PATH"
        return 1
    fi
    return 0
}

# Function to create output directory structure
create_directories() {
    local target=$1
    local output_dir="js_recon_${target}_$(date +%Y%m%d_%H%M%S)"

    mkdir -p "$output_dir"/{raw_output,processed_output,tools_output}
    mkdir -p "$output_dir/tools_output"/{waybackurls,gau,hakrawler,katana,gospider,getjs,subjs,waymore}
    mkdir -p "$output_dir/processed_output"/{urls,js_files,config_files,json_files,sensitive_files}

    echo "$output_dir"
}

# Function to categorize files by extension and sensitivity
categorize_files() {
    local input_file=$1
    local output_dir=$2
    
    # Clear previous categorized files
    > "$output_dir/processed_output/urls/all_urls.txt"
    > "$output_dir/processed_output/js_files/javascript_files.txt"
    > "$output_dir/processed_output/config_files/config_files.txt"
    > "$output_dir/processed_output/json_files/json_files.txt"
    > "$output_dir/processed_output/sensitive_files/potentially_sensitive.txt"

    if [[ -f "$input_file" && -s "$input_file" ]]; then
        # All URLs
        cat "$input_file" >> "$output_dir/processed_output/urls/all_urls.txt"
        
        # JavaScript files
        grep -E "\\.js(\?|$|#)" "$input_file" >> "$output_dir/processed_output/js_files/javascript_files.txt" 2>/dev/null || true
        
        # Configuration files
        grep -iE "\\.(conf|config|cfg|ini|properties|yaml|yml|toml|env)(\?|$|#)" "$input_file" >> "$output_dir/processed_output/config_files/config_files.txt" 2>/dev/null || true
        
        # JSON files
        grep -E "\\.json(\?|$|#)" "$input_file" >> "$output_dir/processed_output/json_files/json_files.txt" 2>/dev/null || true
        
        # Potentially sensitive files (including XML, SQL, backup files, etc.)
        grep -iE "\\.(xml|sql|bak|backup|old|swp|tmp|log|key|pem|p12|pfx|jks|keystore|crt|cer|csr|db|sqlite|dump)(\?|$|#)" "$input_file" >> "$output_dir/processed_output/sensitive_files/potentially_sensitive.txt" 2>/dev/null || true
        
        # Also check for sensitive patterns in URLs
        grep -iE "(api|admin|config|secret|key|token|password|credential|backup|private|internal)" "$input_file" >> "$output_dir/processed_output/sensitive_files/potentially_sensitive.txt" 2>/dev/null || true
    fi
}

# Function to clean and deduplicate files
clean_categorized_files() {
    local output_dir=$1
    
    # Clean and deduplicate each category
    for category_dir in "$output_dir/processed_output"/{urls,js_files,config_files,json_files,sensitive_files}; do
        if [[ -d "$category_dir" ]]; then
            for file in "$category_dir"/*.txt; do
                if [[ -f "$file" ]]; then
                    # Remove fragments, deduplicate, and clean
                    sed 's/#.*//' "$file" | sort -u | grep -v '^$' > "$file.tmp" 2>/dev/null || touch "$file.tmp"
                    mv "$file.tmp" "$file"
                fi
            done
        fi
    done
}

# Function to validate target
validate_target() {
    local target=$1

    if [[ -z "$target" ]]; then
        print_error "No target specified"
        return 1
    fi

    # Add protocol if missing
    if [[ ! "$target" =~ ^https?:// ]]; then
        target="https://$target"
    fi

    echo "$target"
}

# Function to run waybackurls
run_waybackurls() {
    local target=$1
    local output_dir=$2

    print_tool "waybackurls"

    # Extract domain for waybackurls
    domain=$(echo "$target" | sed -E 's|^https?://||' | cut -d'/' -f1)

    timeout $TIMEOUT waybackurls "$domain" > "$output_dir/tools_output/waybackurls/raw_urls.txt" 2>/dev/null

    if [[ -s "$output_dir/tools_output/waybackurls/raw_urls.txt" ]]; then
        categorize_files "$output_dir/tools_output/waybackurls/raw_urls.txt" "$output_dir"
        local count=$(wc -l < "$output_dir/tools_output/waybackurls/raw_urls.txt" 2>/dev/null || echo 0)
        print_success "waybackurls found $count URLs"
    else
        print_warning "waybackurls returned no results"
        touch "$output_dir/tools_output/waybackurls/raw_urls.txt"
    fi
}

# Function to run gau
run_gau() {
    local target=$1
    local output_dir=$2

    print_tool "gau"

    # Extract domain for gau
    domain=$(echo "$target" | sed -E 's|^https?://||' | cut -d'/' -f1)

    timeout $TIMEOUT gau "$domain" --threads 5 > "$output_dir/tools_output/gau/raw_urls.txt" 2>/dev/null

    if [[ -s "$output_dir/tools_output/gau/raw_urls.txt" ]]; then
        categorize_files "$output_dir/tools_output/gau/raw_urls.txt" "$output_dir"
        local count=$(wc -l < "$output_dir/tools_output/gau/raw_urls.txt" 2>/dev/null || echo 0)
        print_success "gau found $count URLs"
    else
        print_warning "gau returned no results"
        touch "$output_dir/tools_output/gau/raw_urls.txt"
    fi
}

# Function to run hakrawler
run_hakrawler() {
    local subdomain_file=$1
    local output_dir=$2

    print_tool "hakrawler"

    if [[ -f "$subdomain_file" ]]; then
        timeout $TIMEOUT bash -c "cat '$subdomain_file' | hakrawler -d 3 -insecure -u" > "$output_dir/tools_output/hakrawler/raw_urls.txt" 2>/dev/null

        if [[ -s "$output_dir/tools_output/hakrawler/raw_urls.txt" ]]; then
            categorize_files "$output_dir/tools_output/hakrawler/raw_urls.txt" "$output_dir"
            local count=$(wc -l < "$output_dir/tools_output/hakrawler/raw_urls.txt" 2>/dev/null || echo 0)
            print_success "hakrawler found $count URLs"
        else
            print_warning "hakrawler returned no results"
            touch "$output_dir/tools_output/hakrawler/raw_urls.txt"
        fi
    else
        print_warning "Subdomain file not found for hakrawler"
        touch "$output_dir/tools_output/hakrawler/raw_urls.txt"
    fi
}

# Function to run katana
run_katana() {
    local subdomain_file=$1
    local output_dir=$2

    print_tool "katana"

    if [[ -f "$subdomain_file" ]]; then
        timeout $TIMEOUT katana -list "$subdomain_file" -d 3 -jc -silent > "$output_dir/tools_output/katana/raw_urls.txt" 2>/dev/null

        if [[ -s "$output_dir/tools_output/katana/raw_urls.txt" ]]; then
            categorize_files "$output_dir/tools_output/katana/raw_urls.txt" "$output_dir"
            local count=$(wc -l < "$output_dir/tools_output/katana/raw_urls.txt" 2>/dev/null || echo 0)
            print_success "katana found $count URLs"
        else
            print_warning "katana returned no results"
            touch "$output_dir/tools_output/katana/raw_urls.txt"
        fi
    else
        print_warning "Subdomain file not found for katana"
        touch "$output_dir/tools_output/katana/raw_urls.txt"
    fi
}

# Function to run gospider
run_gospider() {
    local subdomain_file=$1
    local output_dir=$2

    print_tool "gospider"

    if [[ -f "$subdomain_file" ]]; then
        timeout $TIMEOUT gospider -S "$subdomain_file" -d 0 -c 10 --js -q > "$output_dir/tools_output/gospider/raw_urls.txt" 2>/dev/null

        if [[ -s "$output_dir/tools_output/gospider/raw_urls.txt" ]]; then
            categorize_files "$output_dir/tools_output/gospider/raw_urls.txt" "$output_dir"
            local count=$(wc -l < "$output_dir/tools_output/gospider/raw_urls.txt" 2>/dev/null || echo 0)
            print_success "gospider found $count URLs"
        else
            print_warning "gospider returned no results"
            touch "$output_dir/tools_output/gospider/raw_urls.txt"
        fi
    else
        print_warning "Subdomain file not found for gospider"
        touch "$output_dir/tools_output/gospider/raw_urls.txt"
    fi
}

# Function to run getJS
run_getjs() {
    local subdomain_file=$1
    local output_dir=$2

    print_tool "getJS"

    if [[ -f "$subdomain_file" ]]; then
        # Process each subdomain individually for getJS with timeout
        while IFS= read -r subdomain; do
            if [[ -n "$subdomain" ]]; then
                timeout 60 getJS --url "$subdomain" --complete >> "$output_dir/tools_output/getjs/raw_urls_temp.txt" 2>/dev/null
            fi
        done < "$subdomain_file"

        # Clean up and process results
        if [[ -f "$output_dir/tools_output/getjs/raw_urls_temp.txt" ]]; then
            sort -u "$output_dir/tools_output/getjs/raw_urls_temp.txt" > "$output_dir/tools_output/getjs/raw_urls.txt"
            rm -f "$output_dir/tools_output/getjs/raw_urls_temp.txt"

            categorize_files "$output_dir/tools_output/getjs/raw_urls.txt" "$output_dir"
            local count=$(wc -l < "$output_dir/tools_output/getjs/raw_urls.txt" 2>/dev/null || echo 0)
            print_success "getJS found $count URLs"
        else
            print_warning "getJS returned no results"
            touch "$output_dir/tools_output/getjs/raw_urls.txt"
        fi
    else
        print_warning "Subdomain file not found for getJS"
        touch "$output_dir/tools_output/getjs/raw_urls.txt"
    fi
}

# Function to run subjs
run_subjs() {
    local subdomain_file=$1
    local output_dir=$2

    print_tool "subjs"

    if [[ -f "$subdomain_file" ]]; then
        timeout $TIMEOUT bash -c "cat '$subdomain_file' | subjs" > "$output_dir/tools_output/subjs/raw_urls.txt" 2>/dev/null

        if [[ -s "$output_dir/tools_output/subjs/raw_urls.txt" ]]; then
            categorize_files "$output_dir/tools_output/subjs/raw_urls.txt" "$output_dir"
            local count=$(wc -l < "$output_dir/tools_output/subjs/raw_urls.txt" 2>/dev/null || echo 0)
            print_success "subjs found $count URLs"
        else
            print_warning "subjs returned no results"
            touch "$output_dir/tools_output/subjs/raw_urls.txt"
        fi
    else
        print_warning "Subdomain file not found for subjs"
        touch "$output_dir/tools_output/subjs/raw_urls.txt"
    fi
}

# Function to run waymore
run_waymore() {
    local target=$1
    local output_dir=$2

    print_tool "waymore"

    # Extract domain for waymore
    domain=$(echo "$target" | sed -E 's|^https?://||' | cut -d'/' -f1)

    timeout $TIMEOUT waymore -i "$domain" -mode U -oU "$output_dir/tools_output/waymore/raw_urls.txt" 2>/dev/null

    if [[ -s "$output_dir/tools_output/waymore/raw_urls.txt" ]]; then
        categorize_files "$output_dir/tools_output/waymore/raw_urls.txt" "$output_dir"
        local count=$(wc -l < "$output_dir/tools_output/waymore/raw_urls.txt" 2>/dev/null || echo 0)
        print_success "waymore found $count URLs"
    else
        print_warning "waymore returned no results"
        touch "$output_dir/tools_output/waymore/raw_urls.txt"
    fi
}

# Function to combine and process results
process_results() {
    local output_dir=$1

    print_status "Processing and combining results..."

    # Combine all URLs from all tools
    cat "$output_dir"/tools_output/*/raw_urls.txt > "$output_dir/raw_output/all_raw_urls.txt" 2>/dev/null

    # Final categorization of all combined results
    categorize_files "$output_dir/raw_output/all_raw_urls.txt" "$output_dir"

    # Clean and deduplicate categorized files
    clean_categorized_files "$output_dir"

    # Create statistics
    create_statistics "$output_dir"
}

# Function to create statistics
create_statistics() {
    local output_dir=$1
    local stats_file="$output_dir/processed_output/statistics.txt"

    echo "JavaScript Reconnaissance Statistics" > "$stats_file"
    echo "===================================" >> "$stats_file"
    echo "Scan Date: $(date)" >> "$stats_file"
    echo "Timeout: $TIMEOUT seconds" >> "$stats_file"
    echo "" >> "$stats_file"

    echo "Results by Tool:" >> "$stats_file"
    echo "---------------" >> "$stats_file"

    for tool_dir in "$output_dir"/tools_output/*/; do
        if [[ -d "$tool_dir" ]]; then
            tool_name=$(basename "$tool_dir")
            raw_file="$tool_dir/raw_urls.txt"

            if [[ -f "$raw_file" ]]; then
                count=$(wc -l < "$raw_file" 2>/dev/null || echo 0)
                printf "%-15s: %d URLs\n" "$tool_name" "$count" >> "$stats_file"
            fi
        fi
    done

    echo "" >> "$stats_file"
    echo "Summary by File Type:" >> "$stats_file"
    echo "--------------------" >> "$stats_file"

    local total_urls=$(wc -l < "$output_dir/processed_output/urls/all_urls.txt" 2>/dev/null || echo 0)
    local js_files=$(wc -l < "$output_dir/processed_output/js_files/javascript_files.txt" 2>/dev/null || echo 0)
    local config_files=$(wc -l < "$output_dir/processed_output/config_files/config_files.txt" 2>/dev/null || echo 0)
    local json_files=$(wc -l < "$output_dir/processed_output/json_files/json_files.txt" 2>/dev/null || echo 0)
    local sensitive_files=$(wc -l < "$output_dir/processed_output/sensitive_files/potentially_sensitive.txt" 2>/dev/null || echo 0)

    echo "Total URLs found: $total_urls" >> "$stats_file"
    echo "JavaScript files: $js_files" >> "$stats_file"
    echo "Configuration files: $config_files" >> "$stats_file"
    echo "JSON files: $json_files" >> "$stats_file"
    echo "Potentially sensitive files: $sensitive_files" >> "$stats_file"
}

# Function to display final results
display_results() {
    local output_dir=$1
    local target=$2

    echo -e "\n${GREEN}╔══════════════════════════════════════════════════════════════════════════════╗"
    echo -e "║                               SCAN COMPLETE                                  ║"
    echo -e "╚══════════════════════════════════════════════════════════════════════════════╝${NC}\n"

    print_status "Target: $target"
    print_status "Output Directory: $output_dir"
    print_status "Timeout: $TIMEOUT seconds"

    if [[ -f "$output_dir/processed_output/statistics.txt" ]]; then
        echo -e "\n${CYAN}Statistics:${NC}"
        cat "$output_dir/processed_output/statistics.txt" | tail -n +7
    fi

    echo -e "\n${YELLOW}Categorized Output Files:${NC}"
    echo "• All URLs: $output_dir/processed_output/urls/all_urls.txt"
    echo "• JavaScript files: $output_dir/processed_output/js_files/javascript_files.txt"
    echo "• Configuration files: $output_dir/processed_output/config_files/config_files.txt"
    echo "• JSON files: $output_dir/processed_output/json_files/json_files.txt"
    echo "• Potentially sensitive files: $output_dir/processed_output/sensitive_files/potentially_sensitive.txt"
    echo ""
    echo -e "${YELLOW}Raw Data:${NC}"
    echo "• All raw URLs: $output_dir/raw_output/all_raw_urls.txt"
    echo "• Individual tool outputs: $output_dir/tools_output/"
    echo "• Statistics: $output_dir/processed_output/statistics.txt"
}

# Main function
main() {
    # Check if target and subdomain file are provided
    if [[ $# -lt 2 ]]; then
        echo -e "${RED}Usage: $0 <target_url_or_domain> <subdomain_list_file>${NC}"
        echo -e "Example: $0 https://example.com subdomains.txt"
        echo -e "Example: $0 example.com subdomains.txt"
        echo -e ""
        echo -e "${YELLOW}Note:${NC}"
        echo -e "• Domain-based tools (waybackurls, gau, waymore) will use the main domain"
        echo -e "• Subdomain-based tools (hakrawler, katana, gospider, getjs, subjs) will use the subdomain list"
        echo -e "• Timeout is set to $TIMEOUT seconds (2 hours)"
        exit 1
    fi

    local target=$(validate_target "$1")
    local subdomain_file="$2"

    if [[ $? -ne 0 ]]; then
        exit 1
    fi

    # Check if subdomain file exists
    if [[ ! -f "$subdomain_file" ]]; then
        print_error "Subdomain file '$subdomain_file' not found!"
        exit 1
    fi

    local subdomain_count=$(wc -l < "$subdomain_file" 2>/dev/null || echo 0)
    print_status "Starting comprehensive reconnaissance for: $target"
    print_status "Using subdomain list: $subdomain_file ($subdomain_count subdomains)"
    print_status "Timeout set to: $TIMEOUT seconds"

    # Check required tools (removed crawlergo)
    local missing_tools=()
    local tools=(waybackurls gau hakrawler katana gospider getJS subjs waymore)

    for tool in "${tools[@]}"; do
        if ! check_tool "$tool"; then
            missing_tools+=("$tool")
        fi
    done

    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        print_warning "Some tools are missing: ${missing_tools[*]}"
        print_status "Continuing with available tools..."
    fi

    # Create output directory
    local output_dir=$(create_directories "$(echo "$target" | sed -E 's|^https?://||' | cut -d'/' -f1)")
    print_status "Created output directory: $output_dir"

    # Run tools
    echo -e "\n${CYAN}Running reconnaissance tools...${NC}\n"

    # Domain-based tools (run in background)
    run_waybackurls "$target" "$output_dir" &
    run_gau "$target" "$output_dir" &
    run_waymore "$target" "$output_dir" &

    # Subdomain-based tools (run in background)
    run_hakrawler "$subdomain_file" "$output_dir" &
    run_katana "$subdomain_file" "$output_dir" &
    run_gospider "$subdomain_file" "$output_dir" &

    # Wait for first batch
    wait

    # Continue with remaining subdomain-based tools
    run_getjs "$subdomain_file" "$output_dir" &
    run_subjs "$subdomain_file" "$output_dir" &

    # Wait for final batch
    wait

    # Process results
    echo -e "\n${CYAN}Processing results...${NC}"
    process_results "$output_dir"

    # Display final results
    display_results "$output_dir" "$target"

    print_success "Reconnaissance completed successfully!"
}

# Handle Ctrl+C
trap 'echo -e "\n${RED}[INTERRUPTED]${NC} Scan interrupted by user"; exit 130' INT

# Run main function
main "$@"
